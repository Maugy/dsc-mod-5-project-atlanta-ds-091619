{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:30:42.678102Z",
     "start_time": "2019-12-29T02:30:40.728460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas\n",
    "from datetime import datetime \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from time import sleep, time\n",
    "import lxml\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import unicodedata\n",
    "from collections import defaultdict, Counter\n",
    "import WebScrape_Indeed\n",
    "import WebScrape_LinkedIn\n",
    "import streamlit as st \n",
    "import terms \n",
    "import Cities \n",
    "import functions\n",
    "import time\n",
    "from google.cloud import bigquery, storage\n",
    "from google_pandas_load import Loader, LoaderQuickSetup\n",
    "from google_pandas_load import LoadConfig\n",
    "\n",
    "import chart_studio.plotly \n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "import pandas as pd\n",
    "\n",
    "import pydeck as pdk\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)   \n",
    "\n",
    "import string\n",
    "import collections\n",
    " \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.manifold.t_sne import _joint_probabilities\n",
    "from scipy import linalg\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "palette = sns.color_palette(\"bright\", 10)\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:30:42.685894Z",
     "start_time": "2019-12-29T02:30:42.680430Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import (make_blobs,\n",
    "                                                make_circles,\n",
    "                                                make_moons)\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('notebook')\n",
    "plt.style.use('fivethirtyeight')\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:30:42.690883Z",
     "start_time": "2019-12-29T02:30:42.687536Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()  # for plot styling\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:30:42.945473Z",
     "start_time": "2019-12-29T02:30:42.692569Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:30:42.951820Z",
     "start_time": "2019-12-29T02:30:42.948258Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean(raw):\n",
    "    raw = ' '.join(raw.tolist())\n",
    "    for char in '-.,\\n':\n",
    "        raw = raw.replace(char,' ')\n",
    "    \n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    raw = nltk.regexp_tokenize(raw, pattern)\n",
    "#     raw = str(raw)\n",
    "    return raw\n",
    "\n",
    "def cleanC(raw):\n",
    "#     raw = ' '.join(raw.tolist())\n",
    "    for char in '-.,\\n':\n",
    "        raw = raw.replace(char,' ')\n",
    "    return raw\n",
    "\n",
    "def C_plus(raw):\n",
    "    Cplus = re.findall(r'(?i)\\bC\\+\\+(?!\\w)', str(raw))\n",
    "    return Cplus\n",
    "\n",
    "def C_sharp(raw):\n",
    "    Csharp = re.findall(r'(?i)\\bC\\#(?!\\w)', str(raw))\n",
    "    return Csharp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:30:43.600571Z",
     "start_time": "2019-12-29T02:30:43.595012Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()  # for plot styling\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:30:46.040357Z",
     "start_time": "2019-12-29T02:30:43.993873Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:30:46.292995Z",
     "start_time": "2019-12-29T02:30:46.042622Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Esurance</td>\n",
       "      <td>San Francisco, CA 94105</td>\n",
       "      <td>[     Summary       Esurance is looking for a ...</td>\n",
       "      <td>12-01-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DATA SCIENTIST</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>Sunnyvale, CA 94087</td>\n",
       "      <td>[     Position Description    Data Scientist i...</td>\n",
       "      <td>12-01-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Kohl's</td>\n",
       "      <td>Milpitas, CA 95035</td>\n",
       "      <td>[  Interpret and apply data analyses and expla...</td>\n",
       "      <td>12-01-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Staff Data Scientist (GEC11903)</td>\n",
       "      <td>Walmart eCommerce</td>\n",
       "      <td>Sunnyvale, CA 94087</td>\n",
       "      <td>[     Position Description      Understands an...</td>\n",
       "      <td>12-01-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sr. Data Scientist</td>\n",
       "      <td>Silicon Valley Bank</td>\n",
       "      <td>Palo Alto, CA 94304</td>\n",
       "      <td>[       Silicon Valley Bank is the market lead...</td>\n",
       "      <td>12-01-2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title              company  \\\n",
       "0                   Data Scientist             Esurance   \n",
       "1                   DATA SCIENTIST              Walmart   \n",
       "2                   Data Scientist               Kohl's   \n",
       "3  Staff Data Scientist (GEC11903)    Walmart eCommerce   \n",
       "4               Sr. Data Scientist  Silicon Valley Bank   \n",
       "\n",
       "                  location                                        description  \\\n",
       "0  San Francisco, CA 94105  [     Summary       Esurance is looking for a ...   \n",
       "1      Sunnyvale, CA 94087  [     Position Description    Data Scientist i...   \n",
       "2       Milpitas, CA 95035  [  Interpret and apply data analyses and expla...   \n",
       "3      Sunnyvale, CA 94087  [     Position Description      Understands an...   \n",
       "4      Palo Alto, CA 94304  [       Silicon Valley Bank is the market lead...   \n",
       "\n",
       "         date  \n",
       "0  12-01-2019  \n",
       "1  12-01-2019  \n",
       "2  12-01-2019  \n",
       "3  12-01-2019  \n",
       "4  12-01-2019  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('total_data_date.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:30:47.912403Z",
     "start_time": "2019-12-29T02:30:47.909236Z"
    }
   },
   "outputs": [],
   "source": [
    "search_terms = terms.total_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:30:49.054337Z",
     "start_time": "2019-12-29T02:30:49.051514Z"
    }
   },
   "outputs": [],
   "source": [
    "option = 'Data Scientist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:22.201114Z",
     "start_time": "2019-12-29T02:36:22.181344Z"
    }
   },
   "outputs": [],
   "source": [
    "dtitle = data[data['title'].astype(str).str.contains(option)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:22.596521Z",
     "start_time": "2019-12-29T02:36:22.593774Z"
    }
   },
   "outputs": [],
   "source": [
    "text = dtitle.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:23.472177Z",
     "start_time": "2019-12-29T02:36:23.002378Z"
    }
   },
   "outputs": [],
   "source": [
    "text_clean = clean(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:23.585392Z",
     "start_time": "2019-12-29T02:36:23.581364Z"
    }
   },
   "outputs": [],
   "source": [
    "def tech_count(text):\n",
    "    tech_skills = []\n",
    "    List1 = [x.lower() for x in search_terms]\n",
    "    List2 = [x.lower() for x in text_clean]\n",
    "\n",
    "    for item in List2:\n",
    "        if item in List1:\n",
    "            tech_skills.append(item)\n",
    "        else:\n",
    "            None \n",
    "    return tech_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:26.710438Z",
     "start_time": "2019-12-29T02:36:24.688245Z"
    }
   },
   "outputs": [],
   "source": [
    "tech_list = tech_count(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:26.717405Z",
     "start_time": "2019-12-29T02:36:26.712565Z"
    }
   },
   "outputs": [],
   "source": [
    "tech_series = pd.DataFrame(tech_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:26.726204Z",
     "start_time": "2019-12-29T02:36:26.719932Z"
    }
   },
   "outputs": [],
   "source": [
    "tech_series = tech_series.rename(columns={0:'Tech'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:26.734873Z",
     "start_time": "2019-12-29T02:36:26.727789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bigquery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hadoop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22308</th>\n",
       "      <td>xgboost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22309</th>\n",
       "      <td>spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22310</th>\n",
       "      <td>docker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22311</th>\n",
       "      <td>kubernetes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22312</th>\n",
       "      <td>nlp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22313 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Tech\n",
       "0               r\n",
       "1          python\n",
       "2        bigquery\n",
       "3            hive\n",
       "4          hadoop\n",
       "...           ...\n",
       "22308     xgboost\n",
       "22309       spark\n",
       "22310      docker\n",
       "22311  kubernetes\n",
       "22312         nlp\n",
       "\n",
       "[22313 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tech_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:27.540785Z",
     "start_time": "2019-12-29T02:36:27.296196Z"
    }
   },
   "outputs": [],
   "source": [
    "one = pd.read_csv('Indeed.csv')\n",
    "one = one['description']\n",
    "one = clean(one)\n",
    "\n",
    "two = pd.read_csv('LinkedIn.csv')\n",
    "two = two['description']\n",
    "two = clean(two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:28.421303Z",
     "start_time": "2019-12-29T02:36:28.417674Z"
    }
   },
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(tokenizer=None,\n",
    "#                                  stop_words=stopwords.words('english'),\n",
    "#                                  max_df=1.0,\n",
    "#                                  min_df=0.0,\n",
    "#                                  lowercase=True)\n",
    " \n",
    "\n",
    "# X = vectorizer.fit_transform(dtotal['description'])\n",
    "# X = X.toarray()\n",
    "# y_hat = km.predict(X)\n",
    "\n",
    "# clustering = collections.defaultdict(list)\n",
    " \n",
    "# for idx, label in enumerate(km.labels_):\n",
    "#     clustering[label].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:28.932849Z",
     "start_time": "2019-12-29T02:36:28.929596Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_len = 3+1\n",
    "# text_sequences = []\n",
    "# for i in range(train_len,len(tokens)):\n",
    "#     seq = tokens[i-train_len:i]\n",
    "#     text_sequences.append(seq)\n",
    "\n",
    "# sequences = {}\n",
    "# count = 1\n",
    "# for i in range(len(tokens)):\n",
    "#     if tokens[i] not in sequences:\n",
    "#         sequences[tokens[i]] = count\n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:30.224361Z",
     "start_time": "2019-12-29T02:36:30.220899Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(text_sequences)\n",
    "# sequences = tokenizer.texts_to_sequences(text_sequences) \n",
    "\n",
    "# #Collecting some information   \n",
    "# vocabulary_size = len(tokenizer.word_counts)\n",
    "\n",
    "# n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
    "# for i in range(len(sequences)):\n",
    "#     n_sequences[i] = sequences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:30.599366Z",
     "start_time": "2019-12-29T02:36:30.596526Z"
    }
   },
   "outputs": [],
   "source": [
    "# def create_model(vocabulary_size, seq_len):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocabulary_size, seq_len,input_length=seq_len))\n",
    "#     model.add(LSTM(50,return_sequences=True))\n",
    "#     model.add(LSTM(50))\n",
    "#     model.add(Dense(50,activation='relu'))\n",
    "#     model.add(Dense(vocabulary_size,activation='softmax'))\n",
    "#     opt_adam = optimizers.adam(lr=0.001)\n",
    "#     #You can simply pass 'adam' to optimizer in compile method. Default learning rate 0.001\n",
    "#     #But here we are using adam optimzer from optimizer class to change the LR.\n",
    "#     model.compile(loss='categorical_crossentropy',optimizer=opt_adam,metrics=['accuracy'])\n",
    "#     model.summary()\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:31.050131Z",
     "start_time": "2019-12-29T02:36:31.047478Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = create_model(vocabulary_size+1,seq_len)\n",
    "# path = './checkpoints/word_pred_Model4.h5'\n",
    "# checkpoint = ModelCheckpoint(path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "# model.fit(train_inputs,train_targets,batch_size=128,epochs=500,verbose=1,callbacks=[checkpoint])\n",
    "# dump(tokenizer,open('tokenizer_Model4','wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:36:31.460914Z",
     "start_time": "2019-12-29T02:36:31.457491Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = load_model('word_pred_Model4.h5')\n",
    "# tokenizer = load(open('tokenizer_Model4','rb'))\n",
    "# seq_len = 3 \n",
    "# def gen_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "#     output_text = []\n",
    "#     input_text = seed_text\n",
    "#     for i in range(num_gen_words):\n",
    "#         encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "#         pad_encoded = pad_sequences([encoded_text], maxlen=seq_len,truncating='pre')\n",
    "#         pred_word_ind = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "        \n",
    "#         pred_word = tokenizer.index_word[pred_word_ind]\n",
    "#         input_text += ' '+pred_word\n",
    "#         output_text.append(pred_word)\n",
    "#     return ' '.join(output_text)\n",
    "\n",
    "# print('\\n\\n===>Enter --exit to exit from the program')\n",
    "# while True:\n",
    "#     seed_text  = input('Enter string: ')\n",
    "#     if seed_text.lower() == '--exit':\n",
    "#         break\n",
    "#     else:\n",
    "#         out = gen_text(model, tokenizer, seq_len=seq_len, seed_text=seed_text, num_gen_words=5)\n",
    "#         print('Output: '+seed_text+' '+out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:37:26.430327Z",
     "start_time": "2019-12-29T02:36:40.579727Z"
    }
   },
   "outputs": [],
   "source": [
    "target = data.title\n",
    "data = data['description'].map(word_tokenize).values\n",
    "# data = df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:37:26.435613Z",
     "start_time": "2019-12-29T02:37:26.432530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17424"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:37:27.742218Z",
     "start_time": "2019-12-29T02:37:26.437142Z"
    }
   },
   "outputs": [],
   "source": [
    "new_data = tech_series['Tech'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:40:15.358047Z",
     "start_time": "2019-12-29T02:39:40.507960Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 4697120 words, keeping 43926 word types\n",
      "collected 50097 word types from a corpus of 8524966 raw words and 17424 sentences\n",
      "Loading a fresh vocabulary\n",
      "min_count=5 retains 26063 unique words (52% of original 50097, drops 24034)\n",
      "min_count=5 leaves 8483180 word corpus (99% of original 8524966, drops 41786)\n",
      "deleting the raw counts dictionary of 50097 items\n",
      "sample=0.001 downsamples 36 most-common words\n",
      "downsampling leaves estimated 6471432 word corpus (76.3% of prior 8483180)\n",
      "estimated required memory for 26063 words and 100 dimensions: 33881900 bytes\n",
      "resetting layer weights\n",
      "training model with 4 workers on 26063 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "EPOCH 1 - PROGRESS: at 39.58% examples, 2419112 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 76.38% examples, 2411484 words/s, in_qsize 7, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 8524966 raw words (6471581 effective words) took 2.7s, 2410145 effective words/s\n",
      "EPOCH 2 - PROGRESS: at 38.98% examples, 2385075 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 75.92% examples, 2398860 words/s, in_qsize 7, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 2 : training on 8524966 raw words (6471297 effective words) took 2.8s, 2341836 effective words/s\n",
      "EPOCH 3 - PROGRESS: at 34.91% examples, 2124640 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 67.84% examples, 2118205 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 98.43% examples, 2114195 words/s, in_qsize 7, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 3 : training on 8524966 raw words (6471622 effective words) took 3.1s, 2102310 effective words/s\n",
      "EPOCH 4 - PROGRESS: at 34.67% examples, 2115370 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 68.04% examples, 2130443 words/s, in_qsize 7, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "EPOCH 4 - PROGRESS: at 100.00% examples, 2156260 words/s, in_qsize 0, out_qsize 1\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 4 : training on 8524966 raw words (6471330 effective words) took 3.0s, 2155387 effective words/s\n",
      "EPOCH 5 - PROGRESS: at 36.48% examples, 2227650 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 70.01% examples, 2196310 words/s, in_qsize 7, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 5 : training on 8524966 raw words (6472177 effective words) took 3.0s, 2187526 effective words/s\n",
      "training on a 42624830 raw words (32358007 effective words) took 14.5s, 2230315 effective words/s\n",
      "Effective 'alpha' higher than previous training cycles\n",
      "training model with 4 workers on 26063 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "EPOCH 1 - PROGRESS: at 35.42% examples, 2156267 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 68.38% examples, 2136258 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 99.21% examples, 2130843 words/s, in_qsize 7, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 8524966 raw words (6470941 effective words) took 3.0s, 2130411 effective words/s\n",
      "EPOCH 2 - PROGRESS: at 34.17% examples, 2085833 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 68.04% examples, 2127344 words/s, in_qsize 7, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "EPOCH 2 - PROGRESS: at 99.88% examples, 2150253 words/s, in_qsize 1, out_qsize 1\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 2 : training on 8524966 raw words (6471194 effective words) took 3.0s, 2149594 effective words/s\n",
      "EPOCH 3 - PROGRESS: at 35.65% examples, 2174403 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 69.73% examples, 2181908 words/s, in_qsize 7, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 3 : training on 8524966 raw words (6470215 effective words) took 3.0s, 2167786 effective words/s\n",
      "EPOCH 4 - PROGRESS: at 35.65% examples, 2175664 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 69.52% examples, 2177186 words/s, in_qsize 7, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 4 : training on 8524966 raw words (6471425 effective words) took 3.0s, 2178571 effective words/s\n",
      "EPOCH 5 - PROGRESS: at 34.79% examples, 2114615 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 67.10% examples, 2090473 words/s, in_qsize 7, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 97.29% examples, 2092763 words/s, in_qsize 7, out_qsize 0\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 5 : training on 8524966 raw words (6471954 effective words) took 3.1s, 2090174 effective words/s\n",
      "training on a 42624830 raw words (32355729 effective words) took 15.1s, 2140579 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32355729, 42624830)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# > **Note**: Normally, you would also perform preprocessing steps such as  tokenization before training a Word2Vec model.\n",
    "\n",
    "model = Word2Vec(data, size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:40:15.743696Z",
     "start_time": "2019-12-29T02:40:15.361324Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 10000 words, keeping 79 word types\n",
      "PROGRESS: at sentence #20000, processed 20000 words, keeping 80 word types\n",
      "collected 80 word types from a corpus of 22313 raw words and 22313 sentences\n",
      "Loading a fresh vocabulary\n",
      "min_count=5 retains 71 unique words (88% of original 80, drops 9)\n",
      "min_count=5 leaves 22295 word corpus (99% of original 22313, drops 18)\n",
      "deleting the raw counts dictionary of 80 items\n",
      "sample=0.001 downsamples 46 most-common words\n",
      "downsampling leaves estimated 5752 word corpus (25.8% of prior 22295)\n",
      "estimated required memory for 71 words and 100 dimensions: 92300 bytes\n",
      "resetting layer weights\n",
      "training model with 4 workers on 71 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 22313 raw words (5719 effective words) took 0.0s, 229008 effective words/s\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 2 : training on 22313 raw words (5779 effective words) took 0.0s, 239790 effective words/s\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 3 : training on 22313 raw words (5807 effective words) took 0.0s, 252084 effective words/s\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 4 : training on 22313 raw words (5759 effective words) took 0.0s, 257077 effective words/s\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 5 : training on 22313 raw words (5792 effective words) took 0.0s, 243139 effective words/s\n",
      "training on a 111565 raw words (28856 effective words) took 0.2s, 179417 effective words/s\n",
      "under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "Effective 'alpha' higher than previous training cycles\n",
      "training model with 4 workers on 71 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 22313 raw words (5735 effective words) took 0.0s, 303094 effective words/s\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 2 : training on 22313 raw words (5758 effective words) took 0.0s, 250552 effective words/s\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 3 : training on 22313 raw words (5686 effective words) took 0.0s, 354257 effective words/s\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 4 : training on 22313 raw words (5722 effective words) took 0.0s, 266154 effective words/s\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 5 : training on 22313 raw words (5775 effective words) took 0.0s, 254393 effective words/s\n",
      "training on a 111565 raw words (28676 effective words) took 0.2s, 179796 effective words/s\n",
      "under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28676, 111565)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(new_data, size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "model.train(new_data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:40:15.751446Z",
     "start_time": "2019-12-29T02:40:15.748240Z"
    }
   },
   "outputs": [],
   "source": [
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:40:15.761266Z",
     "start_time": "2019-12-29T02:40:15.754405Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('html', 0.2453547865152359),\n",
       " ('spark', 0.19633592665195465),\n",
       " ('bash', 0.17984220385551453),\n",
       " ('scala', 0.14753027260303497),\n",
       " ('ibm', 0.14402225613594055),\n",
       " ('shell', 0.1408894956111908),\n",
       " ('r', 0.11437661945819855),\n",
       " ('net', 0.11345686763525009),\n",
       " ('nlp', 0.10244505107402802),\n",
       " ('python', 0.09449000656604767)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('gcp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:40:17.508468Z",
     "start_time": "2019-12-29T02:40:15.767002Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 10000 words, keeping 79 word types\n",
      "PROGRESS: at sentence #20000, processed 20000 words, keeping 80 word types\n",
      "collected 80 word types from a corpus of 22313 raw words and 22313 sentences\n",
      "Loading a fresh vocabulary\n",
      "min_count=5 retains 71 unique words (88% of original 80, drops 9)\n",
      "min_count=5 leaves 22295 word corpus (99% of original 22313, drops 18)\n",
      "deleting the raw counts dictionary of 80 items\n",
      "sample=0.001 downsamples 46 most-common words\n",
      "downsampling leaves estimated 5752 word corpus (25.8% of prior 22295)\n",
      "estimated required memory for 71 words and 100 dimensions: 92300 bytes\n",
      "resetting layer weights\n",
      "training model with 4 workers on 71 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 22313 raw words (5719 effective words) took 0.0s, 363968 effective words/s\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 2 : training on 22313 raw words (5781 effective words) took 0.0s, 413160 effective words/s\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 3 : training on 22313 raw words (5761 effective words) took 0.0s, 385406 effective words/s\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 4 : training on 22313 raw words (5756 effective words) took 0.0s, 399039 effective words/s\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 5 : training on 22313 raw words (5706 effective words) took 0.0s, 259684 effective words/s\n",
      "training on a 111565 raw words (28723 effective words) took 0.2s, 188163 effective words/s\n",
      "under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "Effective 'alpha' higher than previous training cycles\n",
      "training model with 4 workers on 71 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 8524966 raw words (1192 effective words) took 0.3s, 3551 effective words/s\n",
      "EPOCH - 1 : supplied example count (17424) did not equal expected count (22313)\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 2 : training on 8524966 raw words (1154 effective words) took 0.3s, 3615 effective words/s\n",
      "EPOCH - 2 : supplied example count (17424) did not equal expected count (22313)\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 3 : training on 8524966 raw words (1159 effective words) took 0.3s, 4088 effective words/s\n",
      "EPOCH - 3 : supplied example count (17424) did not equal expected count (22313)\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 4 : training on 8524966 raw words (1171 effective words) took 0.3s, 4142 effective words/s\n",
      "EPOCH - 4 : supplied example count (17424) did not equal expected count (22313)\n",
      "worker thread finished; awaiting finish of 3 more threads\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 5 : training on 8524966 raw words (1167 effective words) took 0.3s, 4006 effective words/s\n",
      "EPOCH - 5 : supplied example count (17424) did not equal expected count (22313)\n",
      "training on a 42624830 raw words (5843 effective words) took 1.5s, 3814 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5843, 42624830)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(new_data, size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:40:17.512424Z",
     "start_time": "2019-12-29T02:40:17.510526Z"
    }
   },
   "outputs": [],
   "source": [
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T02:40:17.518666Z",
     "start_time": "2019-12-29T02:40:17.514193Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('keras', 0.9414958953857422),\n",
       " ('javascript', 0.9401934146881104),\n",
       " ('bash', 0.9398542642593384),\n",
       " ('pandas', 0.9391921758651733),\n",
       " ('numpy', 0.9377909898757935),\n",
       " ('html', 0.9372595548629761),\n",
       " ('scrum', 0.9370600581169128),\n",
       " ('linux', 0.9370077848434448),\n",
       " ('shell', 0.9367868304252625),\n",
       " ('tensorflow', 0.9364053010940552)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wv.most_similar(negative='java')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['gcp', 'sql'], negative=['python'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
