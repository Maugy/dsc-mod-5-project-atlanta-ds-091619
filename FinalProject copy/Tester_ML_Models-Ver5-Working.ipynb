{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:10:23.027498Z",
     "start_time": "2019-12-23T15:10:19.098782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas\n",
    "from datetime import datetime \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from time import sleep, time\n",
    "import lxml\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import unicodedata\n",
    "from collections import defaultdict, Counter\n",
    "import WebScrape_Indeed\n",
    "import WebScrape_LinkedIn\n",
    "import streamlit as st \n",
    "import terms \n",
    "import Cities \n",
    "import functions\n",
    "import time\n",
    "from google.cloud import bigquery, storage\n",
    "from google_pandas_load import Loader, LoaderQuickSetup\n",
    "from google_pandas_load import LoadConfig\n",
    "\n",
    "import chart_studio.plotly \n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "import pandas as pd\n",
    "\n",
    "import pydeck as pdk\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)   \n",
    "\n",
    "import string\n",
    "import collections\n",
    " \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.manifold.t_sne import _joint_probabilities\n",
    "from scipy import linalg\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "palette = sns.color_palette(\"bright\", 10)\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:10:23.035564Z",
     "start_time": "2019-12-23T15:10:23.029920Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import (make_blobs,\n",
    "                                                make_circles,\n",
    "                                                make_moons)\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('notebook')\n",
    "plt.style.use('fivethirtyeight')\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:10:23.041719Z",
     "start_time": "2019-12-23T15:10:23.037633Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()  # for plot styling\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:10:23.391639Z",
     "start_time": "2019-12-23T15:10:23.043324Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:10:23.399320Z",
     "start_time": "2019-12-23T15:10:23.394862Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean(raw):\n",
    "    raw = ' '.join(raw.tolist())\n",
    "    for char in '-.,\\n':\n",
    "        raw = raw.replace(char,' ')\n",
    "    \n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    raw = nltk.regexp_tokenize(raw, pattern)\n",
    "    raw = str(raw)\n",
    "    return raw\n",
    "\n",
    "def cleanC(raw):\n",
    "    raw = ' '.join(raw.tolist())\n",
    "    for char in '-.,\\n':\n",
    "        raw = raw.replace(char,' ')\n",
    "    return raw\n",
    "\n",
    "def C_plus(raw):\n",
    "    Cplus = re.findall(r'(?i)\\bC\\+\\+(?!\\w)', str(raw))\n",
    "    return Cplus\n",
    "\n",
    "def C_sharp(raw):\n",
    "    Csharp = re.findall(r'(?i)\\bC\\#(?!\\w)', str(raw))\n",
    "    return Csharp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:10:23.406240Z",
     "start_time": "2019-12-23T15:10:23.401701Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()  # for plot styling\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:10:25.414975Z",
     "start_time": "2019-12-23T15:10:23.463582Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:10:25.420001Z",
     "start_time": "2019-12-23T15:10:25.417160Z"
    }
   },
   "outputs": [],
   "source": [
    "search_terms = terms.total_terms\n",
    "search_terms = [term.lower() for term in search_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:10:25.782412Z",
     "start_time": "2019-12-23T15:10:25.614420Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Esurance</td>\n",
       "      <td>San Francisco, CA 94105</td>\n",
       "      <td>[     Summary       Esurance is looking for a ...</td>\n",
       "      <td>12-01-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DATA SCIENTIST</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>Sunnyvale, CA 94087</td>\n",
       "      <td>[     Position Description    Data Scientist i...</td>\n",
       "      <td>12-01-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Kohl's</td>\n",
       "      <td>Milpitas, CA 95035</td>\n",
       "      <td>[  Interpret and apply data analyses and expla...</td>\n",
       "      <td>12-01-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Staff Data Scientist (GEC11903)</td>\n",
       "      <td>Walmart eCommerce</td>\n",
       "      <td>Sunnyvale, CA 94087</td>\n",
       "      <td>[     Position Description      Understands an...</td>\n",
       "      <td>12-01-2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sr. Data Scientist</td>\n",
       "      <td>Silicon Valley Bank</td>\n",
       "      <td>Palo Alto, CA 94304</td>\n",
       "      <td>[       Silicon Valley Bank is the market lead...</td>\n",
       "      <td>12-01-2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             title              company  \\\n",
       "0                   Data Scientist             Esurance   \n",
       "1                   DATA SCIENTIST              Walmart   \n",
       "2                   Data Scientist               Kohl's   \n",
       "3  Staff Data Scientist (GEC11903)    Walmart eCommerce   \n",
       "4               Sr. Data Scientist  Silicon Valley Bank   \n",
       "\n",
       "                  location                                        description  \\\n",
       "0  San Francisco, CA 94105  [     Summary       Esurance is looking for a ...   \n",
       "1      Sunnyvale, CA 94087  [     Position Description    Data Scientist i...   \n",
       "2       Milpitas, CA 95035  [  Interpret and apply data analyses and expla...   \n",
       "3      Sunnyvale, CA 94087  [     Position Description      Understands an...   \n",
       "4      Palo Alto, CA 94304  [       Silicon Valley Bank is the market lead...   \n",
       "\n",
       "         date  \n",
       "0  12-01-2019  \n",
       "1  12-01-2019  \n",
       "2  12-01-2019  \n",
       "3  12-01-2019  \n",
       "4  12-01-2019  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('total_data_date.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:10:45.890853Z",
     "start_time": "2019-12-23T15:10:45.887212Z"
    }
   },
   "outputs": [],
   "source": [
    "target = df.title\n",
    "data = df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:10:49.530551Z",
     "start_time": "2019-12-23T15:10:49.524477Z"
    }
   },
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:11:00.768121Z",
     "start_time": "2019-12-23T15:10:55.961795Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(data))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(data)\n",
    "X_t = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:11:51.996705Z",
     "start_time": "2019-12-23T15:11:51.849087Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "input_ = Input(shape=(100,))\n",
    "x = Embedding(20000, embedding_size)(input_)\n",
    "x = LSTM(25, return_sequences=True)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# There are 41 different possible classes, so we use 41 neurons in our output layer\n",
    "x = Dense(1637, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:11:52.979611Z",
     "start_time": "2019-12-23T15:11:52.948369Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:11:53.828677Z",
     "start_time": "2019-12-23T15:11:53.823624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 25)           15400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 130)               6630      \n",
      "=================================================================\n",
      "Total params: 2,583,330\n",
      "Trainable params: 2,583,330\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T15:11:54.919268Z",
     "start_time": "2019-12-23T15:11:54.897704Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 8363 input samples and 131 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d993a14066da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    800\u001b[0m             ]\n\u001b[1;32m    801\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    234\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 8363 input samples and 131 target samples."
     ]
    }
   ],
   "source": [
    "model.fit(X_t, y, epochs=3, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
