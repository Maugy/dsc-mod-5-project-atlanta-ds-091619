{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas\n",
    "from datetime import datetime \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from time import sleep, time\n",
    "import lxml\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import unicodedata\n",
    "from collections import defaultdict, Counter\n",
    "import WebScrape_Indeed\n",
    "import WebScrape_LinkedIn\n",
    "import streamlit as st \n",
    "import terms \n",
    "import Cities \n",
    "import functions\n",
    "import time\n",
    "from google.cloud import bigquery, storage\n",
    "from google_pandas_load import Loader, LoaderQuickSetup\n",
    "from google_pandas_load import LoadConfig\n",
    "\n",
    "import chart_studio.plotly \n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "import pandas as pd\n",
    "\n",
    "import pydeck as pdk\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)   \n",
    "\n",
    "import string\n",
    "import collections\n",
    " \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.manifold.t_sne import _joint_probabilities\n",
    "from scipy import linalg\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "palette = sns.color_palette(\"bright\", 10)\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()  # for plot styling\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets.samples_generator import make_blobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import (make_blobs,\n",
    "                                                make_circles,\n",
    "                                                make_moons)\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('notebook')\n",
    "plt.style.use('fivethirtyeight')\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()  # for plot styling\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(raw):\n",
    "    raw = ' '.join(raw.tolist())\n",
    "    for char in '-.,\\n':\n",
    "        raw = raw.replace(char,' ')\n",
    "    \n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    raw = nltk.regexp_tokenize(raw, pattern)\n",
    "#     raw = str(raw)\n",
    "    return raw\n",
    "\n",
    "def cleanC(raw):\n",
    "    raw = ' '.join(raw.tolist())\n",
    "    for char in '-.,\\n':\n",
    "        raw = raw.replace(char,' ')\n",
    "    return raw\n",
    "\n",
    "def C_plus(raw):\n",
    "    Cplus = re.findall(r'(?i)\\bC\\+\\+(?!\\w)', str(raw))\n",
    "    return Cplus\n",
    "\n",
    "def C_sharp(raw):\n",
    "    Csharp = re.findall(r'(?i)\\bC\\#(?!\\w)', str(raw))\n",
    "    return Csharp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('total_data_date.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = terms.total_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_terms = [term.lower() for term in search_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = str(search_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data['description'].str.lower().str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = clean(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.description.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = str(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = df['description'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unique_data = []\n",
    "# for i in range(len(data)):\n",
    "#     if data['description'][i] not in unique_data:\n",
    "#         unique_data.append(data['description'][i])\n",
    "#         if i % 5000 == 0:\n",
    "#             print('{0}'.format(i)+' lines have been processed')\n",
    "#     else:\n",
    "#         None\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() #convert all the chracters into small letters\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ve\", \"have\", text)\n",
    "    text = re.sub(r\"\\'ll\", \"will\", text)\n",
    "    text = re.sub(r\"\\'re\", \"are\", text)\n",
    "    text = re.sub(r\"\\'d\", \"would\", text)\n",
    "    text = re.sub(r\"n't\", \"not\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}'+=|.!?,]\", \"\", text)\n",
    "    text = text.replace(\"[\", \"\")\n",
    "    text = text.replace(\"]\", \"\")\n",
    "    return text\n",
    "\n",
    "# # unique_data_str = []\n",
    "# # for i in range(len(unique_data)):\n",
    "# #     if type(unique_data[i]) is str:\n",
    "# #         unique_data_str.append(unique_data[i])\n",
    "# #     else:\n",
    "# #         None\n",
    "\n",
    "# # Cleaning the data\n",
    "# clean_data = []\n",
    "# for text in data.description:\n",
    "#     a = re.sub(r'[^a-zA-z ]+', '', text).strip()\n",
    "#     if len(a)>0:\n",
    "#         clean_data.append(clean_text(a))\n",
    "#     else:\n",
    "#         None\n",
    "\n",
    "# tech_skills = []\n",
    "# for word in clean_data:\n",
    "#     if word in search_terms:\n",
    "#         tech_skills.append(word)\n",
    "#     else:\n",
    "#         None\n",
    "\n",
    "# tech_skills = [x.split(',')[0] for x in dblocation if isinstance(x, str)]\n",
    "\n",
    "# word2count = {}\n",
    "# total_words = 0\n",
    "# for text in tech_skills:\n",
    "#     for word in text.split():\n",
    "#         if word not in word2count:\n",
    "#             word2count[word] = 1\n",
    "#         else:\n",
    "#             word2count[word] += 1\n",
    "#         total_words += 1\n",
    "\n",
    "        \n",
    "# Removing the lines which are to short or to long\n",
    "# short_data = []\n",
    "# for line in clean_data:\n",
    "#     if 2 <= len(line.split()):\n",
    "#         short_data.append(line)\n",
    "#     else:\n",
    "#         None\n",
    "\n",
    "# Counting the appearnce of each word in the corpus also calculates the number of unique words also\n",
    "# word2count = {}\n",
    "# total_words = 0\n",
    "# for text in clean_data:\n",
    "#     for word in text.split():\n",
    "#         if word not in word2count:\n",
    "#             word2count[word] = 1\n",
    "#         else:\n",
    "#             word2count[word] += 1\n",
    "#         total_words += 1\n",
    "\n",
    "\n",
    "# creating a list that will only contain the words that appear more than 15 times\n",
    "# word10 = []\n",
    "# threshold = 10\n",
    "# for word, count in word2count.items():\n",
    "#     if count >= threshold:\n",
    "#         if len(word) > 1:\n",
    "#             word10.append(word)\n",
    "            \n",
    "# Removing the words from each string which appear less than 15 times\n",
    "# data10 = []\n",
    "# for line in short_data:\n",
    "#     str1=''\n",
    "#     for word in line.split():\n",
    "#         if word in word10:\n",
    "#             str1 = \" \".join((str1, word))\n",
    "#     data10.append(str1)\n",
    "\n",
    "# Removing the lines which are to short or to long after removing the unnecssary words.     \n",
    "# short_data_consize = []\n",
    "# for line in data10:\n",
    "#     if 3 <= len(line.split()):\n",
    "#         short_data_consize.append(line)\n",
    "#     else:\n",
    "#         None\n",
    "        \n",
    "# clean_unique_data = []\n",
    "# for i in range(len(short_data_consize)):\n",
    "#     if short_data_consize[i] not in clean_unique_data:\n",
    "#         clean_unique_data.append(short_data_consize[i])\n",
    "#     else:\n",
    "#         None\n",
    "        \n",
    "# # Total number of words in corpus after removing the words which appears less than 15 times and further cleaning\n",
    "# total_words_d10 = 0\n",
    "# for line in data10:\n",
    "#     for word in line.split():\n",
    "#         total_words_d10 += 1 \n",
    "# # \"\"\" Initially we had 437579 lines in our data after cleaning and preprocessing the data\n",
    "#     now our complete data will have 126003 lines, that means we removed 71.20% data which was useless\"\"\"   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tech_count(text):\n",
    "    tech_skills = []\n",
    "    List1 = [x.lower() for x in search_terms]\n",
    "    List2 = [x.lower() for x in text]\n",
    "\n",
    "    for item in List2:\n",
    "        if item in List1:\n",
    "            tech_skills.append(item)\n",
    "        else:\n",
    "            None \n",
    "    return tech_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tech_count(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splitting the cleaned and preprocessd data into 4 equal parts      \n",
    "# clean_unique_data_qtr1 = clean_unique_data[0:int(len(clean_unique_data)*0.25)]      \n",
    "# clean_unique_data_qtr2 = clean_unique_data[int(len(clean_unique_data)*0.25):int(len(clean_unique_data)*0.5)]  \n",
    "# clean_unique_data_qtr3 = clean_unique_data[int(len(clean_unique_data)*0.5):int(len(clean_unique_data)*0.75)]  \n",
    "# clean_unique_data_qtr4 = clean_unique_data[int(len(clean_unique_data)*0.75):len(clean_unique_data)]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop = stopwords.words('english') + list(string.punctuation)\n",
    "# stop += [\"''\", '\"\"', '...', '``','--','and','a','an','of']\n",
    "\n",
    "# new_data = []\n",
    "# for word in str(data).split():\n",
    "#     if word not in stop:\n",
    "#         new_data.append(word)\n",
    "        \n",
    "# data = data.map(word_tokenize).values\n",
    "# tokens = str(data).split()\n",
    "# stopwords_removed = [token for token in tokens if token not in stop]\n",
    "# stopwords_removed = [x for x in data if x not in stop]\n",
    "# data = data.apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_again = []\n",
    "# for text in new_data:\n",
    "#     a = re.sub(r'[^a-zA-z ]+', '', text).strip()\n",
    "#     if len(a)>0:\n",
    "#         clean_again.append(clean_text(a))\n",
    "#     else:\n",
    "#         None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 3+1\n",
    "text_sequences = []\n",
    "for i in range(train_len,len(d)):\n",
    "    seq = d[i-train_len:i]\n",
    "    text_sequences.append(seq)\n",
    "\n",
    "sequences = {}\n",
    "count = 1\n",
    "for i in range(len(d)):\n",
    "    if d[i] not in sequences:\n",
    "        sequences[d[i]] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop = stopwords.words('english') + list(string.punctuation)\n",
    "# stop += [\"''\", '\"\"', '...', '``','--']\n",
    "# data = data.apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences) \n",
    "\n",
    "#Collecting some information   \n",
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "\n",
    "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
    "for i in range(len(sequences)):\n",
    "    n_sequences[i] = sequences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_inputs = n_sequences[:,:-1]\n",
    "train_targets = n_sequences[:,-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_targets = train_targets.astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_targets = to_categorical(train_targets, num_classes=vocabulary_size+1)\n",
    "seq_len = train_inputs.shape[1]\n",
    "train_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
    "# It is a flexible layer that can be used in a variety of ways, such as:\n",
    "# It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
    "# It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
    "# It can be used to load a pre-trained word embedding model, a type of transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, seq_len,input_length=seq_len))\n",
    "    model.add(LSTM(50,return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(50,activation='relu'))\n",
    "    model.add(Dense(vocabulary_size,activation='softmax'))\n",
    "    opt_adam = optimizers.adam(lr=0.01)\n",
    "    #You can simply pass 'adam' to optimizer in compile method. Default learning rate 0.001\n",
    "    #But here we are using adam optimzer from optimizer class to change the LR.\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt_adam,metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(vocabulary_size+1,seq_len)\n",
    "path = 'checkpoint6'\n",
    "checkpoint = ModelCheckpoint(path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(train_inputs,train_targets,batch_size=128,epochs=50,verbose=1,callbacks=[checkpoint])\n",
    "# pickle.dump(tokenizer,open('tokenizer_Model4','wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model('checkpoint5')\n",
    "# tokenizer = pickle.load(open('tokenizer_Model4','rb'))\n",
    "seq_len = 3 \n",
    "def gen_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    for i in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len,truncating='pre')\n",
    "        pred_word_ind = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "        \n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        input_text += ' '+pred_word\n",
    "        for word in pred_word:\n",
    "            if word not in output_text:\n",
    "                output_text.append(pred_word)\n",
    "        \n",
    "#         auxiliaryList = []\n",
    "#         for word in output_text:\n",
    "#             if word not in auxiliaryList:\n",
    "#                 auxiliaryList.append(word)\n",
    "    return ' '.join(output_text)\n",
    "\n",
    "print('\\n\\n===>Enter --exit to exit from the program')\n",
    "while True:\n",
    "    seed_text  = input('Enter string: ')\n",
    "    if seed_text.lower() == '--exit':\n",
    "        break\n",
    "    else:\n",
    "        out = gen_text(model, tokenizer, seq_len=seq_len, seed_text=seed_text, num_gen_words=5)\n",
    "        print('Output: '+seed_text+' '+out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(string):\n",
    "    freq = defaultdict(int)\n",
    "    List1 = [x.lower() for x in search_terms]\n",
    "    List2 = [x.lower() for x in string]\n",
    "\n",
    "    for item in List1:\n",
    "        if item in List2:\n",
    "            freq[item] = List2.count(item)\n",
    "        else:\n",
    "            freq[item] = 0 \n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('checkpoint5')\n",
    "# tokenizer = pickle.load(open('tokenizer_Model4','rb'))\n",
    "seq_len = 3 \n",
    "def gen_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    for i in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len,truncating='pre')\n",
    "        pred_word_ind = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "        \n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        input_text += ' '+pred_word\n",
    "        output_text.append(pred_word)\n",
    "    return ' '.join(output_text)\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n===>Enter --exit to exit from the program')\n",
    "while True:\n",
    "    seed_text  = input('Enter string: ')\n",
    "    if seed_text.lower() == '--exit':\n",
    "        break\n",
    "    else:\n",
    "        out = gen_text(model, tokenizer, seq_len=seq_len, seed_text=seed_text, num_gen_words=10)\n",
    "        print('Output: '+seed_text+' '+out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('checkpoint4')\n",
    "# # tokenizer = pickle.load(open('tokenizer_Model4','rb'))\n",
    "# seq_len = 3 \n",
    "# def gen_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "#     output_text = []\n",
    "#     input_text = seed_text\n",
    "#     for i in range(num_gen_words):\n",
    "#         encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "#         pad_encoded = pad_sequences([encoded_text], maxlen=seq_len,truncating='pre')\n",
    "#         pred_word_ind = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "        \n",
    "#         pred_word = tokenizer.index_word[pred_word_ind]\n",
    "#         targets = str()\n",
    "#         for word in pred_word:\n",
    "#             if word in search_terms:\n",
    "#                 targets+word\n",
    "                \n",
    "#         input_text += ' '+targets\n",
    "#         output_text.append(targets)\n",
    "#     return ' '.join(output_text)\n",
    "\n",
    "# print('\\n\\n===>Enter --exit to exit from the program')\n",
    "# while True:\n",
    "#     seed_text  = input('Enter string: ')\n",
    "#     if seed_text.lower() == '--exit':\n",
    "#         break\n",
    "#     else:\n",
    "#         out = gen_text(model, tokenizer, seq_len=seq_len, seed_text=seed_text, num_gen_words=5)\n",
    "#         print('Output: '+seed_text+' '+out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = df.title\n",
    "# data = df['description'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_target = set(target)\n",
    "# target_classes = len(total_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_vocabulary = set(word for description in data for word in description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(total_vocabulary)\n",
    "# print(\"There are {} unique tokens in the dataset.\".format(len(total_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove = {}\n",
    "# with open('glove.6B.50d.txt', 'rb') as f:\n",
    "#     for line in f:\n",
    "#         parts = line.split()\n",
    "#         word = parts[0].decode('utf-8')\n",
    "#         if word in total_vocabulary:\n",
    "#             vector = np.array(parts[1:], dtype=np.float32)\n",
    "#             glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove['java']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class W2vVectorizer(object):\n",
    "    \n",
    "#     def __init__(self, w2v):\n",
    "#         # takes in a dictionary of words and vectors as input\n",
    "#         self.w2v = w2v\n",
    "#         if len(w2v) == 0:\n",
    "#             self.dimensions = 0\n",
    "#         else:\n",
    "#             self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "#     # Note from Mike: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "#     # It can't be used in a sklearn Pipeline. \n",
    "#     def fit(self, X, y):\n",
    "#         return self\n",
    "            \n",
    "#     def transform(self, X):\n",
    "#         return np.array([\n",
    "#             np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "#                    or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
