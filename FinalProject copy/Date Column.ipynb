{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T19:12:10.120853Z",
     "start_time": "2019-12-15T19:12:10.106878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Web Scraping - LinkedIn\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from time import sleep, time\n",
    "import lxml\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import unicodedata\n",
    "\n",
    "from datetime import datetime \n",
    "\n",
    "\n",
    "# cit = input('Please, enter a city:\\n')\n",
    "# stat = input('Please, enter a state:\\n')\n",
    "# city = str(cit.replace(' ', '+'))\n",
    "# state = str(stat.replace(' ','+'))\n",
    "# location = city+\"%2C%20\"+state\n",
    "\n",
    "# print(f'Searching {city},{state}. Please wait...')\n",
    "\n",
    "# search_url = \"https://www.linkedin.com/jobs/search?keywords=Data%20Scientist&location=\"+location+\"&trk=guest_job_search_jobs-search-bar_search-submit&redirect=false&position=1&pageNum=0&f_TP=1%2C2\"\n",
    "\n",
    "data = {'title': [],\n",
    "        'company': [], \n",
    "        'location': [],\n",
    "        'description': [],\n",
    "        'date': [],}\n",
    "\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    clean = re.sub(cleanr, ' ', str(raw_html))\n",
    "    cleaner = clean.strip()\n",
    "    cleantext = re.sub('\\n', ' ', cleaner)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def desc_clean(text):\n",
    "    clean = unicodedata.normalize(\"NFKD\", str(text))\n",
    "    testy = clean.replace('\\\\xa0','')\n",
    "    return testy\n",
    "\n",
    "\n",
    "def export_table(data):\n",
    "    table = pd.DataFrame(data, columns=['title', 'company', 'location', 'description', 'date'])\n",
    "    table.index = table.index + 1\n",
    "    # table.to_csv('/users/dmauger/Flatiron/FinalProject/' + 'total_data.csv', mode='a', encoding='utf-8', index=False)\n",
    "    # gpl.load(source='dataframe', destination='bq', data_name='total_data', dataframe=table, write_disposition='WRITE_APPEND', compress=False)\n",
    "    return table \n",
    "    \n",
    "    # print('Scraping done. Here are the results:')\n",
    "    # print(table.info())\n",
    "\n",
    "def export_storage(table):\n",
    "    table.to_csv('/users/dmauger/Flatiron/FinalProject/' + 'total_data_date.csv', mode='a', encoding='utf-8', index=False)\n",
    "    # gpl.load(source='local', destination='bq', data_name='total_data', write_disposition='WRITE_APPEND',bq_schema=bq_schema)\n",
    "    print('Process completed.')\n",
    "\n",
    "def job_details(job):\n",
    "    \n",
    "    sleep(1)\n",
    "\n",
    "    r = requests.get(job)\n",
    "    r.encoding = 'utf-8'\n",
    "\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    info = soup.find('div', class_='topcard__content')\n",
    "\n",
    "    try:\n",
    "        title = info.find('h1').text\n",
    "    except:\n",
    "        title = 'NaN'\n",
    "\n",
    "    try:\n",
    "        company = soup.find('div', class_='topcard__content').find_all('span')[0].text\n",
    "    except:\n",
    "        company = 'NaN'\n",
    "\n",
    "    try:\n",
    "        location = info.find('h3', class_='topcard__flavor-row').find_all('span')[1].text\n",
    "    except:\n",
    "        location = 'NaN'\n",
    "\n",
    "    try:\n",
    "        description = soup.find('div', class_=\"description__text\")\n",
    "    except:\n",
    "        description = soup.find('div', class_=\"description__text\").find_all('p')\n",
    "\n",
    "    data['title'].append(cleanhtml(title))\n",
    "    data['company'].append(cleanhtml(company))\n",
    "    data['location'].append(cleanhtml(location))\n",
    "    data['description'].append(desc_clean(cleanhtml(description)))\n",
    "    data['date'] = pd.to_datetime('today')\n",
    "\n",
    "\n",
    "def extract_title(li_url):\n",
    "\n",
    "    sleep(1)\n",
    "    page = requests.get(li_url)\n",
    "    page.encoding = 'utf-8'\n",
    "    bs = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    for div in bs.find('div', class_=\"results__container\").find_all('li'):\n",
    "        for a in div.find_all('a', attrs={'href': re.compile(\"^https://www.linkedin.com/jobs/\")}):\n",
    "            links.append(a['href'])\n",
    "\n",
    "    for job in links:\n",
    "        job_details(job)\n",
    "\n",
    "    return export_table(data)\n",
    "\n",
    "\n",
    "\n",
    "# extract_title(search_url)\n",
    "\n",
    "\n",
    "# cit = input('Please, enter a city:\\n')\n",
    "# stat = input('Please, enter a state:\\n')\n",
    "# city = str(cit.replace(' ', '+'))\n",
    "# state = str(stat.replace(' ','+'))\n",
    "# location = city+\"%2C%20\"+state\n",
    "\n",
    "# print(f'Searching {city},{state}. Please wait...')\n",
    "\n",
    "# search_url = \"https://www.linkedin.com/jobs/search?keywords=Data%20Scientist&location=\"+location+\"&trk=guest_job_search_jobs-search-bar_search-submit&redirect=false&position=1&pageNum=0&f_TP=1%2C2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T19:12:22.449138Z",
     "start_time": "2019-12-15T19:12:22.445630Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://www.linkedin.com/jobs/search?keywords=Data%20Engineer&location=Atlanta%2C%20Georgia%2C%20United%20States&trk=guest_job_search_jobs-search-bar_search-submit&redirect=false&position=1&pageNum=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T19:13:10.381471Z",
     "start_time": "2019-12-15T19:12:23.257776Z"
    }
   },
   "outputs": [],
   "source": [
    "test2 = extract_title(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T19:13:45.443938Z",
     "start_time": "2019-12-15T19:13:45.423048Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Cox Automotive Inc.</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>We are looking for a Data Engineer to join the...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>TechTalent Squared</td>\n",
       "      <td>Norcross, Georgia</td>\n",
       "      <td>TechTalent Squared is seeking a Data Engineer ...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Auth0</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>Auth0 is a pre-IPO unicorn. We are growing rap...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Applied Resource Group</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "      <td>Location: Atlanta, GA           Do you want to...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>eHire</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>Data Engineer  Apply Today!  Do you enjoy help...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Luxoft</td>\n",
       "      <td>Alpharetta, GA, US</td>\n",
       "      <td>Description    Project Description About the P...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>New Relic, Inc.</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>Your  Opportunity   We are looking for a Data ...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>CapTech Ventures, Inc</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>Company Description    CapTech is an award-win...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Kforce Inc</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>Responsibilities    Kforce has a client seekin...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Jobot</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>This Jobot Job is hosted by Greg Smith Are you...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Data Engineer Consultant</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>We Are:   Applied Intelligence, the people who...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data science Tagging and Analytics Engineer</td>\n",
       "      <td>UPS</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>We are looking for a passionate Digital Market...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Integration Engineer</td>\n",
       "      <td>Cetera Financial Group</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "      <td>Position Title : Data Integration Engineer, Ce...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Bellhops</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>Bellhops offers customers a modern alternative...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Wikimedia Foundation</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>Summary    Wikipedia is where the world turns ...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Quoine</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>Based in our Atlanta office, we are looking fo...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Data Engineer - Data Analyst</td>\n",
       "      <td>SPIN Strategy</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>Data Engineer - Data Analyst   Atlanta, GA   T...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sr. Data Engineer</td>\n",
       "      <td>Inspire Brands</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>Purpose Of The Position    We are looking for ...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Parallel</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>Position Summary    As a senior member of the ...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>SOLTECH</td>\n",
       "      <td>Kennesaw, GA, US</td>\n",
       "      <td>SOLTECH   Staffing Solutions   At SOLTECH, we ...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Associate, Data Engineer</td>\n",
       "      <td>KPMG US</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>Requisition Number:  47021 - 26    Description...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Lead Data Engineer</td>\n",
       "      <td>AnswerRocket</td>\n",
       "      <td>Greater Atlanta Area</td>\n",
       "      <td>AnswerRocket is a leader in AI-powered analyti...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Decooda</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>The Data Engineer is responsible for supportin...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>aquesst</td>\n",
       "      <td>Atlanta, GA, US</td>\n",
       "      <td>Data Engineer Atlanta, GA (Midtown) Long-term ...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Data Feature Engineer</td>\n",
       "      <td>Kimberly-Clark</td>\n",
       "      <td>Roswell, GA, US</td>\n",
       "      <td>Data Feature Engineer    Job Description    Le...</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-12-15 14:13:10.377599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title                 company  \\\n",
       "1                                 Data Engineer     Cox Automotive Inc.   \n",
       "2                                 Data Engineer      TechTalent Squared   \n",
       "3                                 Data Engineer                   Auth0   \n",
       "4                                 Data Engineer  Applied Resource Group   \n",
       "5                                 Data Engineer                   eHire   \n",
       "6                                 Data Engineer                  Luxoft   \n",
       "7                                 Data Engineer         New Relic, Inc.   \n",
       "8                                 Data Engineer   CapTech Ventures, Inc   \n",
       "9                                 Data Engineer              Kforce Inc   \n",
       "10                                Data Engineer                   Jobot   \n",
       "11                     Data Engineer Consultant               Accenture   \n",
       "12  Data science Tagging and Analytics Engineer                     UPS   \n",
       "13                    Data Integration Engineer  Cetera Financial Group   \n",
       "14                                Data Engineer                Bellhops   \n",
       "15                                Data Engineer    Wikimedia Foundation   \n",
       "16                                Data Engineer                  Quoine   \n",
       "17                 Data Engineer - Data Analyst           SPIN Strategy   \n",
       "18                            Sr. Data Engineer          Inspire Brands   \n",
       "19                                Data Engineer                Parallel   \n",
       "20                                Data Engineer                 SOLTECH   \n",
       "21                     Associate, Data Engineer                 KPMG US   \n",
       "22                           Lead Data Engineer            AnswerRocket   \n",
       "23                                Data Engineer                 Decooda   \n",
       "24                                Data Engineer                 aquesst   \n",
       "25                        Data Feature Engineer          Kimberly-Clark   \n",
       "26                                          NaN                     NaN   \n",
       "27                                          NaN                     NaN   \n",
       "28                                          NaN                     NaN   \n",
       "\n",
       "                location                                        description  \\\n",
       "1        Atlanta, GA, US  We are looking for a Data Engineer to join the...   \n",
       "2      Norcross, Georgia  TechTalent Squared is seeking a Data Engineer ...   \n",
       "3        Atlanta, GA, US  Auth0 is a pre-IPO unicorn. We are growing rap...   \n",
       "4       Atlanta, Georgia  Location: Atlanta, GA           Do you want to...   \n",
       "5        Atlanta, GA, US  Data Engineer  Apply Today!  Do you enjoy help...   \n",
       "6     Alpharetta, GA, US  Description    Project Description About the P...   \n",
       "7        Atlanta, GA, US  Your  Opportunity   We are looking for a Data ...   \n",
       "8        Atlanta, GA, US  Company Description    CapTech is an award-win...   \n",
       "9        Atlanta, GA, US  Responsibilities    Kforce has a client seekin...   \n",
       "10       Atlanta, GA, US  This Jobot Job is hosted by Greg Smith Are you...   \n",
       "11       Atlanta, GA, US  We Are:   Applied Intelligence, the people who...   \n",
       "12       Atlanta, GA, US  We are looking for a passionate Digital Market...   \n",
       "13      Atlanta, Georgia  Position Title : Data Integration Engineer, Ce...   \n",
       "14       Atlanta, GA, US  Bellhops offers customers a modern alternative...   \n",
       "15       Atlanta, GA, US  Summary    Wikipedia is where the world turns ...   \n",
       "16       Atlanta, GA, US  Based in our Atlanta office, we are looking fo...   \n",
       "17       Atlanta, GA, US  Data Engineer - Data Analyst   Atlanta, GA   T...   \n",
       "18       Atlanta, GA, US  Purpose Of The Position    We are looking for ...   \n",
       "19       Atlanta, GA, US  Position Summary    As a senior member of the ...   \n",
       "20      Kennesaw, GA, US  SOLTECH   Staffing Solutions   At SOLTECH, we ...   \n",
       "21       Atlanta, GA, US  Requisition Number:  47021 - 26    Description...   \n",
       "22  Greater Atlanta Area  AnswerRocket is a leader in AI-powered analyti...   \n",
       "23       Atlanta, GA, US  The Data Engineer is responsible for supportin...   \n",
       "24       Atlanta, GA, US  Data Engineer Atlanta, GA (Midtown) Long-term ...   \n",
       "25       Roswell, GA, US  Data Feature Engineer    Job Description    Le...   \n",
       "26                   NaN                                               None   \n",
       "27                   NaN                                               None   \n",
       "28                   NaN                                               None   \n",
       "\n",
       "                         date  \n",
       "1  2019-12-15 14:13:10.377599  \n",
       "2  2019-12-15 14:13:10.377599  \n",
       "3  2019-12-15 14:13:10.377599  \n",
       "4  2019-12-15 14:13:10.377599  \n",
       "5  2019-12-15 14:13:10.377599  \n",
       "6  2019-12-15 14:13:10.377599  \n",
       "7  2019-12-15 14:13:10.377599  \n",
       "8  2019-12-15 14:13:10.377599  \n",
       "9  2019-12-15 14:13:10.377599  \n",
       "10 2019-12-15 14:13:10.377599  \n",
       "11 2019-12-15 14:13:10.377599  \n",
       "12 2019-12-15 14:13:10.377599  \n",
       "13 2019-12-15 14:13:10.377599  \n",
       "14 2019-12-15 14:13:10.377599  \n",
       "15 2019-12-15 14:13:10.377599  \n",
       "16 2019-12-15 14:13:10.377599  \n",
       "17 2019-12-15 14:13:10.377599  \n",
       "18 2019-12-15 14:13:10.377599  \n",
       "19 2019-12-15 14:13:10.377599  \n",
       "20 2019-12-15 14:13:10.377599  \n",
       "21 2019-12-15 14:13:10.377599  \n",
       "22 2019-12-15 14:13:10.377599  \n",
       "23 2019-12-15 14:13:10.377599  \n",
       "24 2019-12-15 14:13:10.377599  \n",
       "25 2019-12-15 14:13:10.377599  \n",
       "26 2019-12-15 14:13:10.377599  \n",
       "27 2019-12-15 14:13:10.377599  \n",
       "28 2019-12-15 14:13:10.377599  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T19:18:47.984329Z",
     "start_time": "2019-12-15T19:18:47.974781Z"
    }
   },
   "outputs": [],
   "source": [
    "test2.to_csv('/users/dmauger/Flatiron/FinalProject/' + 'total_data_date.csv', mode='a', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T19:18:53.268648Z",
     "start_time": "2019-12-15T19:18:53.143688Z"
    }
   },
   "outputs": [],
   "source": [
    "total2 = pd.read_csv('total_data_date.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T19:20:08.594173Z",
     "start_time": "2019-12-15T19:20:08.583650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8958 entries, 0 to 8957\n",
      "Data columns (total 5 columns):\n",
      "title          8889 non-null object\n",
      "company        8452 non-null object\n",
      "location       8452 non-null object\n",
      "description    8958 non-null object\n",
      "date           8958 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 350.0+ KB\n"
     ]
    }
   ],
   "source": [
    "total2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T19:06:23.197046Z",
     "start_time": "2019-12-15T19:06:23.188743Z"
    }
   },
   "outputs": [],
   "source": [
    "total['date'] = pd.to_datetime('today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T19:20:34.587990Z",
     "start_time": "2019-12-15T19:20:34.087775Z"
    }
   },
   "outputs": [],
   "source": [
    "total2.to_csv('/users/dmauger/Flatiron/FinalProject/' + 'total_data.csv', mode='a', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T19:20:52.794565Z",
     "start_time": "2019-12-15T19:20:52.695619Z"
    }
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 4 fields in line 8931, saw 5\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a88713719dd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 4 fields in line 8931, saw 5\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('total_data.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
